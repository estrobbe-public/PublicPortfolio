Microservices Demo with LoadGenerator (Web Store and Locust)
https://www.skills.google/paths/12/course_templates/783/labs/612115

Perform the following tasks:

Use Cloud Monitoring to detect issues.
Use Cloud Logging to troubleshoot an application running on GKE.
The demo application used in the lab
To use a concrete example, you will troubleshoot a sample microservices demo app deployed to a GKE cluster. In this demo app, there are many microservices and dependencies among them. You will generate traffic using a loadgenerator and then use Logging, Monitoring, and GKE to notice the error (alert/metrics), identify a root cause with Logging, and then fix/confirm the issue is fixed with Logging and Monitoring.

Cloud Logging architecture diagram

Setup and requirements
Before you click the Start Lab button
Read these instructions. Labs are timed and you cannot pause them. The timer, which starts when you click Start Lab, shows how long Google Cloud resources are made available to you.

This hands-on lab lets you do the lab activities in a real cloud environment, not in a simulation or demo environment. It does so by giving you new, temporary credentials you use to sign in and access Google Cloud for the duration of the lab.

To complete this lab, you need:

Access to a standard internet browser (Chrome browser recommended).
Note: Use an Incognito (recommended) or private browser window to run this lab. This prevents conflicts between your personal account and the student account, which may cause extra charges incurred to your personal account.
Time to complete the labâ€”remember, once you start, you cannot pause a lab.
Note: Use only the student account for this lab. If you use a different Google Cloud account, you may incur charges to that account.
How to start your lab and sign in to the Google Cloud console
Click the Start Lab button. If you need to pay for the lab, a dialog opens for you to select your payment method. On the left is the Lab Details pane with the following:

The Open Google Cloud console button
Time remaining
The temporary credentials that you must use for this lab
Other information, if needed, to step through this lab
Click Open Google Cloud console (or right-click and select Open Link in Incognito Window if you are running the Chrome browser).

The lab spins up resources, and then opens another tab that shows the Sign in page.

Tip: Arrange the tabs in separate windows, side-by-side.

Note: If you see the Choose an account dialog, click Use Another Account.
If necessary, copy the Username below and paste it into the Sign in dialog.

student-01-557f1e7ff2cf@qwiklabs.net
Copied!
You can also find the Username in the Lab Details pane.

Click Next.

Copy the Password below and paste it into the Welcome dialog.

enJxoCKfBZtQ
Copied!
You can also find the Password in the Lab Details pane.

Click Next.

Important: You must use the credentials the lab provides you. Do not use your Google Cloud account credentials.
Note: Using your own Google Cloud account for this lab may incur extra charges.
Click through the subsequent pages:

Accept the terms and conditions.
Do not add recovery options or two-factor authentication (because this is a temporary account).
Do not sign up for free trials.
After a few moments, the Google Cloud console opens in this tab.

Note: To access Google Cloud products and services, click the Navigation menu or type the service or product name in the Search field. Navigation menu icon and Search field
Activate Cloud Shell
Cloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Cloud Shell provides command-line access to your Google Cloud resources.

Click Activate Cloud Shell Activate Cloud Shell icon at the top of the Google Cloud console.

Click through the following windows:

Continue through the Cloud Shell information window.
Authorize Cloud Shell to use your credentials to make Google Cloud API calls.
When you are connected, you are already authenticated, and the project is set to your Project_ID, qwiklabs-gcp-01-b390f77d9358. The output contains a line that declares the Project_ID for this session:

Your Cloud Platform project in this session is set to qwiklabs-gcp-01-b390f77d9358
gcloud is the command-line tool for Google Cloud. It comes pre-installed on Cloud Shell and supports tab-completion.

(Optional) You can list the active account name with this command:
gcloud auth list
Copied!
Click Authorize.
Output:

ACTIVE: *
ACCOUNT: student-01-557f1e7ff2cf@qwiklabs.net

To set the active account, run:
    $ gcloud config set account `ACCOUNT`
(Optional) You can list the project ID with this command:
gcloud config list project
Copied!
Output:

[core]
project = qwiklabs-gcp-01-b390f77d9358
Note: For full documentation of gcloud, in Google Cloud, refer to the gcloud CLI overview guide.
Task 1. Perform infrastructure setup
Connect to a Google Kubernetes Engine cluster and validate that it's been created correctly.

Use the following command to see the cluster's status:
gcloud container clusters list
Copied!
The cluster status will say PROVISIONING.

Wait a moment and run the above command again until the status is RUNNING. This could take several minutes.

Verify that the cluster named central has been created.

You can also monitor the progress in the Cloud console by navigating to Navigation menu > Kubernetes Engine > Clusters.

Once your cluster has a RUNNING status, run the following command to get the cluster credentials:
gcloud container clusters get-credentials central --zone us-west1-a
Copied!
Output:

Fetching cluster endpoint and auth data.
kubeconfig entry generated for central.
Run the following command to verify that the nodes have been created:
kubectl get nodes
Copied!
Your output should resemble the following.

Output:

NAME                                       STATUS    ROLES     AGE       VERSION
gke-central-default-pool-5ff4130f-qz8v    Ready    <none>   24d   v1.27.2-gke.1200
gke-central-default--pool-5ff4130f-ssd2   Ready    <none>   24d   v1.27.2-gke.1200
gke-central-default--pool-5ff4130f-tz63   Ready    <none>   24d   v1.27.2-gke.1200
gke-central-default--pool-5ff4130f-zfmn   Ready    <none>   24d   v1.27.2-gke.1200
</none></none></none></none>
Enable Gemini Code Assist in the Cloud Shell IDE
You can use Gemini Code Assist in an integrated development environment (IDE) such as Cloud Shell to receive guidance on code or solve problems with your code. Before you can start using Gemini Code Assist, you need to enable it.

In Cloud Shell, enable the Gemini for Google Cloud API with the following command:
gcloud services enable cloudaicompanion.googleapis.com
Copied!
Click Open Editor on the Cloud Shell toolbar.
Note: To open the Cloud Shell Editor, click Open Editor on the Cloud Shell toolbar. You can switch between Cloud Shell and the code Editor by clicking Open Editor or Open Terminal, as required.
Click Cloud Code - No Project in the status bar at the bottom of the screen.

Authorize the plugin if necessary. If a project is not automatically selected, click Select a Google Cloud Project, and choose qwiklabs-gcp-01-b390f77d9358.

Verify that your Google Cloud project (qwiklabs-gcp-01-b390f77d9358) displays in the Cloud Code status message in the status bar.

Task 2. Deploy an application
Next, deploy a microservices application called Hipster Shop to your cluster to create a workload you can monitor.

Run the following command to clone the repo:
git clone https://github.com/xiangshen-dk/microservices-demo.git
Copied!
Change to the microservices-demo directory with the following command:
cd microservices-demo
Copied!
In the Cloud Shell Editor's file Explorer, navigate to microservices-demo > release > kubernetes-manifests.yaml.
You can use the AI-powered features of Gemini Code Assist to make changes to your code directly in your code editor. In this instance, you decide to let Gemini Code Assist help explain the kubernetes-manifests.yaml file to support the onboarding of a new member in your team.

Open the kubernetes-manifests.yaml file. This action enables Gemini Code Assist, as indicated by the presence of the Gemini Code Assist: Smart Actions icon in the upper-right corner of the editor.

Click the Gemini Code Assist: Smart Actions Gemini Code Assist: Smart Actions icon and select Explain this.

Gemini Code Assist opens a chat pane with the prefilled prompt of Explain this. In the inline text box of the Code Assist chat, replace the prefilled prompt with the following, and click Send:

As a Kubernetes Architect at Cymbal AI, provide a formal and comprehensive explanation of the kubernetes-manifests.yaml file for new team member onboarding.

Your explanation should:

* Detail the key components used in the configuration file
* Describe key Services and their functions
* Describe the common configuration elements
* Describe what the configuration deploys

For the suggested improvements, don't update this file.
Copied!
The explanation for the code in the kubernetes-manifests.yaml file appears in the Gemini Code Assist chat.

Run the following command to install the app using kubectl:
kubectl apply -f release/kubernetes-manifests.yaml
Copied!
Run the following command to confirm everything is running correctly:
kubectl get pods
Copied!
The output should look similar to the output below.

Output:

NAME                                     READY     STATUS      RESTARTS     AGE
adservice-55f94cfd9c-4lvml               1/1       Running     0            20m
cartservice-6f4946f9b8-6wtff             1/1       Running     2            20m
checkoutservice-5688779d8c-l6crl         1/1       Running     0            20m
currencyservice-665d6f4569-b4sbm         1/1       Running     0            20m
emailservice-684c89bcb8-h48sq            1/1       Running     0            20m
frontend-67c8475b7d-vktsn                1/1       Running     0            20m
loadgenerator-6d646566db-p422w           1/1       Running     0            20m
paymentservice-858d89d64c-hmpkg          1/1       Running     0            20m
productcatalogservice-bcd85cb5-d6xp4     1/1       Running     0            20m
recommendationservice-685d7d6cd9-pxd9g   1/1       Running     0            20m
redis-cart-9b864d47f-c9xc6               1/1       Running     0            20m
shippingservice-5948f9fb5c-vndcp         1/1       Running     0            20m
Rerun the command until all pods are reporting a Running status before proceeding to the next step.
Click Check my progress to verify the objective.
Assessment Completed!
Deploy an application
Assessment Completed!

Run the following command to get the external IP of the application. This command only returns an IP address once the service has been deployed, so you may need to repeat the command until there's an external IP address assigned:
export EXTERNAL_IP=$(kubectl get service frontend-external | awk 'BEGIN { cnt=0; } { cnt+=1; if (cnt > 1) print $4; }')
Copied!
Finally, execute the following command to confirm that the app is up and running:
curl -o /dev/null -s -w "%{http_code}\n"  http://$EXTERNAL_IP
Copied!
Your confirmation should resemble the following output.

Output:

200
After the application is deployed, you can also go to the Cloud console and view the status.

In the Kubernetes Engine > Workloads page, you'll see that all of the pods are OK.

The Workloads page

Now, select Gateways, Services & Ingress, and then click on the Services tab to verify all services are OK. Stay on this screen to set up monitoring for the application.
Task 3. Open the application
Scroll down to frontend-external and click the Endpoints IP of the service.
The Services and Ingress page displaying the highlighted frontend-external IP address

It should open the application to display a page like the following:

The Online Boutique web page displaying product tiles

Task 4. Create a logs-based metric
In this task, you configure Cloud Logging to create a logs-based metric, which is a custom metric in Cloud Monitoring made from log entries. Logs-based metrics are good for counting the number of log entries and tracking the distribution of a value in your logs.

In this case, you use the logs-based metric to count the number of errors in your frontend service. You can then use the metric in both dashboards and alerting.

Return to the Cloud console, and from the Navigation menu, open Logging, then click Logs Explorer.
The Logs Explorer page

Enable Show query and in the Query builder box, add the following query:
resource.type="k8s_container"
severity=ERROR
labels."k8s-pod/app": "recommendationservice"
Copied!
The Query builder page displaying the three lines in the query above

Click Run Query.
The query you are using lets you find all errors from the frontend pod. However, you shouldn't see any results now since there are no errors yet.

To create the logs-based metric, click the Actions dropdown, and select Create Metric.
The Create metric button displayed on the UI

Name the metric Error_Rate_SLI, and click Create Metric to save the log-based metric:
The Create logs metric dialog displaying the populated Log metric name field

The metric is now listed under User-defined Metrics on the Logs-based Metrics page.

Click Check my progress to verify the objective.
Assessment Completed!
Create a logs-based metric
Assessment Completed!

Task 5. Create an alerting policy
Alerting gives timely awareness to problems in your cloud applications so you can resolve the problems quickly.

In this task, you use Cloud Monitoring to monitor your frontend service availability by creating an alerting policy based on the frontend errors logs-based metric that you created previously. When the condition of the alerting policy is met, Cloud Monitoring creates and displays an incident in the Cloud console.

In the Navigation menu, open Monitoring, then click Alerting.

After the workspace is created, click Create Policy at the top.

Note: If required, click Try It! to use the updated alert creation flow.
Click on the Select a metric dropdown. Deselect the Active checkbox.

In the filter by resource and metric name field, type Error_Rate.

Click on Kubernetes Container > Logs-Based Metric. Select logging/user/Error_Rate_SLI and click Apply.

Your screen should look like this:

The Select a metric page

Set Rolling windows function to Rate.

Click Next.

Set 0.5 as your Threshold value.

As expected, there are no failures, and your application is meeting its availability Service Level Objective (SLO).

Click Next again.

Disable Use notification channel.

Provide an alert name such as Error Rate SLI then click Next.

Review the alert and click Create Policy.

Note: You will not create a notification channel for this lab but you should do it for your applications running in production, which allows you to send notifications in ways such as email, mobile app, SMS, Pub/Sub, and webhooks.
Click Check my progress to verify the objective.
Assessment Completed!
Create an alerting policy
Assessment Completed!

Trigger an application error
In this section, you use a load generator to create some traffic for your web application. Since there is a bug that has been intentionally introduced into this version of the application, a certain amount of traffic volume triggers errors. You work through the steps to identify and fix the bug.

From the Navigation menu, select Kubernetes Engine, then Gateways, Services & Ingress, and click the Services tab.

Find the loadgenerator-external service, then click on the endpoints link.

The Services and Ingress page open on the Services tabbed page, which displays the highlighted loadgenerator-external service and endpoints link.

Alternatively, you can open a new browser tab or window, copy/paste the IP to the URL field, for example: http://\[loadgenerator-external-ip\].

You should now be on the Locust load generator page:

The Locust load generator page

Locust is an open-source load generator, which allows you to load test a web app. It can simulate a number of users simultaneously hitting your application endpoints at a certain rate.

Simulate 300 users hitting the app with a hatch rate of 30. Locust adds 30 users per second until it reaches 300 users.

For the host field, you use the frontend-external. Copy the URL from the Gateways, Services & Ingress page; be sure to exclude the port. For example:

The Start new Locust swarm page displaying the Start swarming button

Click the Start swarming button. You should have about 300 users to hit the predefined URLs in a few seconds.
The Statistics page displaying the list of 300 users

Click on the Failures tab to see that there are failures starting to occur. You can see there are a large number of 500-errors.
The Failures tabbed page

Meanwhile, if you click any product on the home page, it's either noticeably slow or you receive errors like the following if you click on a product:

The Online Boutique displaying the HTTP Status error: 500 internal server error.

Confirm the alert and application errors
In the console, from the Navigation menu, click Monitoring, then Alerting. You should see an incident soon regarding logging/user/Error_Rate_SLI. If you don't see an incident right away, wait a minute or two and refresh your page. It can take up to 5 minutes for the alert to fire.

Click the link of the incident:

The Alerting page displaying the incident link in the Incidents section

It brings you to the details page.

In the Logs section, click View in Logs Explorer and select the project ID from the dropdown to view pod logs.
The Incident metrics page displaying the highlighted View logs button

You can also click the Error label in the Logs field explorer panel to only query the errors.
Alternatively, you can click into the Query preview field to show the query builder, then click the Severity dropdown, add Error to the query. Click the Add button, then click Run Query. The dropdown menu allows adding multiple severity values.

The result either way is adding severity=ERROR to your query. Once you do that, you should have all the errors for the recommendationservice pod.

The Logs Explorer page open on the Query builder tabbed page, displaying a list of errors in the Query results section

View the error details by expanding an error event. For example:
The expanded Connect Failed query result

Expand the textPayload.

Click the error message and select Add field to summary line to have the error messages appearing as a summary field:

The Add field to summary line option hihglighted in the expanded error message menu

From there, you can confirm there are indeed many errors for the RecommendationService service. Based on the error messages, it appears the RecommendationService couldn't connect to some downstream services to either get products or recommendations. However, it's still not clear what the root cause is for the errors.

If you revisit the architecture diagram, the RecommendationService provides a list of recommendations to the Frontend services. However, both the Frontend service and the RecommendationService invoke ProductCatalogService for a list of products.

The architecture diagram with the highlighted ProductCatalogService and RecomendationService categories.

For the next step, you will look at the metrics of the main suspect, the ProductCatalogService, for any anomalies. Regardless, you can drill down in the logs to get some insights.

Troubleshoot using the Kubernetes dashboard & logs
One of the first places that you can look at the metrics is the Kubernetes Engine section of the Monitoring console (Navigation menu > Monitoring> Dashboards > GKE).

View the Workloads section.

Navigate to Kubernetes Engine > Workloads > productcatalogservice. You can see the pod for the service is constantly crashing and restarting.

The Active Revisions section highlighted on the Deployment details page

Next, see if there is anything interesting in the logs.

There are 2 ways to easily get to your container logs:

Click on the Logs tab to get a quick view of the most recent logs. Next, click the external link button in the upper right corner of the logs panel to go back to the Logs Explorer.
The Logs tabbed page

In the overview page, click the Container logs link on the Deployment Details page.
The Container logs link highlighted on the Deployment Details page

You are on the Logs Explorer page again, now with a predefined query specifically filtered for the logs from the container you were viewing in GKE.

From the Log Viewer, both the log messages and the histogram show the container is repeatedly parsing product catalogs within a short period of time. It seems very inefficient.

At the bottom of the query results, there might also be a runtime error like the following one:

panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation
This could actually be causing the pod to crash.

To better understand the reason, search the log message in the code.

In Cloud Shell terminal, run the following command:
grep -nri 'successfully parsed product catalog json' src
Copied!
Your output should look like the following, which has the source file name with a line number.

Output:

src/productcatalogservice/server.go:237:        log.Info("successfully parsed product catalog json")
To view the source file, by clicking the Open Editor button in the Cloud Shell menu, then Open in New Window (if you see the Unable to load code editor because third-party cookies are disabled error, click the eye at the top of the Chrome page).
The Open Editor button highlighted in the UI

Click the file microservices-demo/src/productcatalogservice/server.go, scroll down to line 237, and you will find the readCatalogFile method logs this message:
The message: log.Info("successfully parsed product catalog json") return nil

With a little more effort, you can see that if the boolean variable reloadCatalog is true, the service reloads and parses the product catalog each time it's invoked, which seems unnecessary.

If you search the reloadCatalog variable in the code, you can see it's controlled by the environment variable ENABLE_RELOAD and writes a log message for its state.

The log message for the reloadCatalog state

With the server.go file open and Gemini Code Assist enabled in the IDE, notice the presence of the Gemini Code Assist: Smart Actions icon in the upper-right corner of the editor.
In this instance, you decide to ask Gemini Code Assist for help explaining the server implementation to your new team member.

Click the Gemini Code Assist: Smart Actions Gemini Code Assist: Smart Actions icon and select Explain this.

Gemini Code Assist opens a chat pane with the prefilled prompt of Explain this. In the inline text box of the Code Assist chat, replace the prefilled prompt with the following, and click Send:

You are a Kubernetes Architect at Cymbal AI. A new team member is unfamiliar with this server implementation. Explain this server.go file in detail, breaking down its key components used in the code. 

For the suggested improvements, don't update this file.
Copied!
The explanation for the code in the server.go file appears in the Gemini Code Assist chat.

Check the logs again by adding a message to your query and determine if there are any entries that exist.

Return to the tab where Logs Explorer is open and add the following line to the query:
jsonPayload.message:"catalog reloading"
Copied!
So the full query in your query builder is as follows:

resource.type="k8s_container"
resource.labels.location="us-west1-a"
resource.labels.cluster_name="central"
resource.labels.namespace_name="default"
labels.k8s-pod/app="productcatalogservice"
jsonPayload.message:"catalog reloading"
Copied!
Click Run Query again and find an "Enable catalog reloading" message in the container log. This confirms that the catalog reloading feature is enabled.
The Enable catalog reloading message in the container log

At this point you can be certain the frontend error is caused by the overhead to load the catalog for every request. When you increased the load, the overhead caused the service to fail and generate the error.

Task 6. Fix the issue and verify the result
Based on the code and what you're seeing in the logs, you can try to fix the issue by disabling catalog reloading.

In this task, you remove the ENABLE_RELOAD environment variable for the product catalog service. Once you make the variable changes, you redeploy the application and verify that the changes have addressed the observed issue.

Click the Open Terminal button to return to the Cloud Shell terminal if it has closed.

Run the following command:

grep -A1 -ni ENABLE_RELOAD release/kubernetes-manifests.yaml
Copied!
The output shows the line number of the environment variable in the manifest file.

Output:

373:        - name: ENABLE_RELOAD
374-          value: "1"
Delete those two lines to disable the reloading by running the following command:
sed -i -e '373,374d' release/kubernetes-manifests.yaml
Copied!
Then run the following command to reapply the manifest file:
kubectl apply -f release/kubernetes-manifests.yaml
Copied!
You should notice that only the productcatalogservice is configured. The other services are unchanged.

Return to the Deployment detail page (Navigation menu > Kubernetes Engine > Workloads > productcatalogservice), and wait until the pod runs successfully. Wait 2-3 minutes or until you can confirm it stops crashing.
The Deployment details page displaying the highlighted Active revisions section

If you click the Container logs link again, note that the repeating successfully parsing the catalog json messages are gone:
The Query builder page

If you go back to the webapp URL and click the products on the home page, it's also much more responsive and you shouldn't encounter any HTTP errors.

Go back to the load generator, click the Reset Stats button in the top right. The failure percentage is reset and you should not see it increasing anymore.

The failure percentage displaying 0 percent

All these checks indicate that the issue is fixed. If you are still seeing the 500-error, wait another couple of minutes and try clicking on a product again.

Congratulations!
You used Cloud Logging and Cloud Monitoring to find an error in an intentionally misconfigured version of the microservices demo app. This is a similar troubleshooting process that you would use to narrow down issues for your GKE apps in a production environment.

First, you deployed the app to GKE and then set up a metric and alert for frontend errors. Next, you generated a load and then noticed that the alert was triggered. From the alert, you narrowed down the issue to particular services using Cloud Logging. Then, you used Cloud Monitoring and the GKE UI to look at the metrics for the GKE services. To fix the issue, you then deployed an updated configuration to GKE and confirmed that the fix addressed the errors in the logs.

Next steps / Learn more
This lab is based on this blog post on using Logging for your apps running on GKE.
The follow-up post on how DevOps teams can use Cloud Monitoring and Logging to find issues quickly might also be interesting to read.
